# Phase 2-2: æƒ…å¢ƒè®°å¿†æŠ€æœ¯æ–¹æ¡ˆï¼ˆv2 ä¿®è®¢ç‰ˆï¼‰

> **ä¿®è®¢è¯´æ˜**ï¼šåŸºäº Codex è¯„å®¡åé¦ˆï¼Œè°ƒæ•´ä¸ºåˆ†é˜¶æ®µå®æ–½ï¼Œå…³è”æ”¹å¼‚æ­¥ï¼Œå¢åŠ æ ¡å‡†æœºåˆ¶ã€‚

## ğŸ“‹ è®¾è®¡ç›®æ ‡

è®©è®°å¿†ç³»ç»Ÿä»"æˆ‘è®°ä½äº†ä»€ä¹ˆ"å‡çº§åˆ°"æˆ‘åœ¨ä»€ä¹ˆæƒ…å¢ƒä¸‹è®°ä½äº†ä»€ä¹ˆ"ã€‚

| èƒ½åŠ› | å½“å‰ | Phase 2.2a | Phase 2.2b |
|------|------|------------|------------|
| æ—¶é—´æ„ŸçŸ¥ | âŒ | âœ… æ—¶é—´æ ‡ç­¾ | âœ… |
| æƒ…ç»ªæ ‡è®° | âŒ | âœ… æ•ˆä»·+ç´§æ€¥åº¦ | âœ… |
| å…³è”è®°å¿† | âŒ | âŒ | âœ… å¼‚æ­¥å»ºç«‹ |
| æƒ…å¢ƒæ£€ç´¢ | âŒ | âœ… | âœ… |

---

## ğŸ”„ åˆ†é˜¶æ®µå®æ–½

### Phase 2.2aï¼šæƒ…å¢ƒå­—æ®µ + æ£€ç´¢ï¼ˆåŒæ­¥ï¼Œ~2å°æ—¶ï¼‰

**èŒƒå›´**ï¼š
- æ•°æ®åº“æ‰©å±•ï¼ˆ4 å­—æ®µï¼‰
- ContextExtractorï¼ˆå¢å¼ºç‰ˆï¼‰
- `memory_contextual_search` MCP Tool
- `memory_parse_context` MCP Tool

**ä¸åŒ…å«**ï¼šå…³è”è®°å¿†ï¼ˆç§»è‡³ 2.2bï¼‰

### Phase 2.2bï¼šå¼‚æ­¥å…³è” + æ ¡å‡†ï¼ˆ~2å°æ—¶ï¼‰

**èŒƒå›´**ï¼š
- MemoryLinkerï¼ˆå¼‚æ­¥é˜Ÿåˆ—ï¼‰
- é˜ˆå€¼æ ¡å‡†è„šæœ¬
- åŒå‘é“¾æ¥æœºåˆ¶
- è§‚æµ‹æŒ‡æ ‡

---

## ğŸ—„ï¸ æ•°æ®åº“æ‰©å±•

### æ–°å¢å­—æ®µ

```sql
-- Phase 2-2: æƒ…å¢ƒè®°å¿†å­—æ®µ
ALTER TABLE memory_items ADD COLUMN context_tags_json TEXT NOT NULL DEFAULT '[]';
ALTER TABLE memory_items ADD COLUMN emotional_valence REAL NOT NULL DEFAULT 0.0;
ALTER TABLE memory_items ADD COLUMN urgency_level INTEGER NOT NULL DEFAULT 0;  -- ğŸ†• ç‹¬ç«‹ç´§æ€¥åº¦
ALTER TABLE memory_items ADD COLUMN related_json TEXT NOT NULL DEFAULT '[]';   -- ğŸ†• æ”¹ä¸ºå¸¦æƒé‡
ALTER TABLE memory_items ADD COLUMN session_id TEXT;

-- ç´¢å¼•
CREATE INDEX IF NOT EXISTS idx_memory_emotional ON memory_items(emotional_valence);
CREATE INDEX IF NOT EXISTS idx_memory_urgency ON memory_items(urgency_level);
CREATE INDEX IF NOT EXISTS idx_memory_session ON memory_items(session_id);
```

### å­—æ®µè¯´æ˜ï¼ˆä¿®è®¢ï¼‰

| å­—æ®µ | ç±»å‹ | è¯´æ˜ | ä¿®è®¢å†…å®¹ |
|------|------|------|----------|
| `context_tags_json` | TEXT | **ç»Ÿä¸€æ ¼å¼**ï¼š`["time:xxx", "task:xxx", "lang:xxx"]` | ğŸ†• ç»Ÿä¸€å‘½åç©ºé—´ |
| `emotional_valence` | REAL | æƒ…æ„Ÿæ•ˆä»· -1.0 ~ +1.0 | ä¸å˜ |
| `urgency_level` | INT | ç´§æ€¥åº¦ 0-3ï¼ˆ0=æ™®é€šï¼Œ3=P0ï¼‰ | ğŸ†• ä» valence åˆ†ç¦» |
| `related_json` | TEXT | `[{"id": "xxx", "score": 0.65}]` | ğŸ†• å¸¦æƒé‡ |
| `session_id` | TEXT | æ¥æºä¼šè¯ï¼ˆéœ€æ–‡æ¡£åŒ–æ ¼å¼ï¼‰ | ğŸ†• å¢åŠ è§„èŒƒ |

### session_id è§„èŒƒ

```
æ ¼å¼: <channel>:<session_key>
ç¤ºä¾‹: 
  - "openclaw:main"
  - "codex:019c2298-262d-7561-b4bb-eb8db0912467"
  - "manual:cli"
```

---

## ğŸ§  ä¸Šä¸‹æ–‡æå–å™¨ï¼ˆå¢å¼ºç‰ˆï¼‰

### æ–‡ä»¶ï¼š`memtool/context/extractor.py`

**å…³é”®æ”¹è¿›**ï¼š
1. ç»Ÿä¸€æ ‡ç­¾å‘½åç©ºé—´
2. å¦å®šè¯†åˆ«
3. ç´§æ€¥åº¦ç‹¬ç«‹ç»´åº¦
4. çŸ­æ–‡æœ¬å…œåº•

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""æƒ…å¢ƒæå–å™¨ v2ï¼šå¢å¼ºç‰ˆï¼Œå«å¦å®šè¯†åˆ«"""

from __future__ import annotations
import datetime as dt
import re
from typing import Dict, List, Tuple, Optional

# ğŸ†• ç»Ÿä¸€æ ‡ç­¾å¸¸é‡ï¼ˆæå–å™¨å’Œè§£æå™¨å…±äº«ï¼‰
class ContextTags:
    # æ—¶é—´
    TIME_WORK_HOURS = "time:work_hours"
    TIME_LATE_NIGHT = "time:late_night"
    TIME_EARLY_MORNING = "time:early_morning"
    TIME_EVENING = "time:evening"
    TIME_WEEKEND = "time:weekend"
    
    # ä»»åŠ¡
    TASK_DEBUGGING = "task:debugging"
    TASK_API_DESIGN = "task:api_design"
    TASK_DATA_MODEL = "task:data_model"
    TASK_REFACTOR = "task:refactor"
    TASK_TESTING = "task:testing"
    TASK_DEPLOYMENT = "task:deployment"
    
    # æƒ…ç»ª
    EMOTION_POSITIVE = "emotion:positive"
    EMOTION_NEGATIVE = "emotion:negative"
    
    # è¯­è¨€
    LANG_ZH = "lang:zh"
    LANG_EN = "lang:en"


class ContextExtractor:
    """è‡ªåŠ¨æå–è®°å¿†çš„ä¸Šä¸‹æ–‡æ ‡ç­¾å’Œæƒ…æ„Ÿæ•ˆä»·"""
    
    # ğŸ†• å¦å®šè¯ï¼ˆä¸­è‹±åŒè¯­ï¼‰
    NEGATION_WORDS = [
        "not", "no", "never", "didn't", "don't", "won't", "can't", "failed to",
        "æ²¡", "æœª", "ä¸", "æ— æ³•", "æ²¡æœ‰", "ä¸èƒ½", "æœªèƒ½", "æ²¡æå®š"
    ]
    
    # æƒ…ç»ªå…³é”®è¯åº“
    EMOTIONAL_KEYWORDS = {
        "positive": [
            "success", "solved", "fixed", "completed", "optimized", "improved",
            "æˆåŠŸ", "è§£å†³", "ä¿®å¤", "å®Œæˆ", "ä¼˜åŒ–", "æ”¹è¿›", "æå®š", "é€šè¿‡"
        ],
        "negative": [
            "error", "failed", "bug", "issue", "timeout", "crash", "exception",
            "é”™è¯¯", "å¤±è´¥", "é—®é¢˜", "è¶…æ—¶", "å´©æºƒ", "å¼‚å¸¸", "æŠ¥é”™", "å¡ä½"
        ],
    }
    
    # ğŸ†• ç´§æ€¥åº¦å…³é”®è¯ï¼ˆç‹¬ç«‹ç»´åº¦ï¼‰
    URGENCY_KEYWORDS = {
        3: ["P0", "critical", "blocking", "ç´§æ€¥", "é˜»å¡", "é©¬ä¸Š"],
        2: ["P1", "urgent", "asap", "é‡è¦", "ä¼˜å…ˆ"],
        1: ["P2", "soon", "å°½å¿«"],
    }
    
    # ä»»åŠ¡ç±»å‹å…³é”®è¯
    TASK_KEYWORDS = {
        ContextTags.TASK_DEBUGGING: ["debug", "trace", "stack", "è°ƒè¯•", "æ’æŸ¥", "å®šä½"],
        ContextTags.TASK_API_DESIGN: ["api", "endpoint", "rest", "graphql", "æ¥å£"],
        ContextTags.TASK_DATA_MODEL: ["schema", "database", "table", "migration", "æ•°æ®åº“", "è¡¨ç»“æ„"],
        ContextTags.TASK_REFACTOR: ["refactor", "cleanup", "é‡æ„", "æ•´ç†", "ä¼˜åŒ–ç»“æ„"],
        ContextTags.TASK_TESTING: ["test", "unittest", "pytest", "æµ‹è¯•", "ç”¨ä¾‹"],
        ContextTags.TASK_DEPLOYMENT: ["deploy", "release", "docker", "k8s", "éƒ¨ç½²", "å‘å¸ƒ"],
    }
    
    WORK_HOURS = (9, 18)
    MIN_CONTENT_LENGTH = 10  # ğŸ†• çŸ­æ–‡æœ¬é˜ˆå€¼
    
    @classmethod
    def extract(
        cls,
        content: str,
        metadata: Optional[Dict] = None,
        timestamp: Optional[dt.datetime] = None
    ) -> Tuple[List[str], float, int]:
        """
        æå–ä¸Šä¸‹æ–‡æ ‡ç­¾ã€æƒ…æ„Ÿæ•ˆä»·ã€ç´§æ€¥åº¦
        
        Returns:
            (context_tags, emotional_valence, urgency_level)
        """
        metadata = metadata or {}
        now = timestamp or dt.datetime.now()
        
        # ğŸ†• çŸ­æ–‡æœ¬å…œåº•
        if len(content.strip()) < cls.MIN_CONTENT_LENGTH:
            return ([], 0.0, 0)
        
        tags = []
        valence = 0.0
        urgency = 0
        
        content_lower = content.lower()
        
        # 1. æ—¶é—´ä¸Šä¸‹æ–‡
        tags.extend(cls._extract_time_context(now))
        
        # 2. æƒ…ç»ªæ£€æµ‹ï¼ˆå«å¦å®šè¯†åˆ«ï¼‰
        emotion_tags, valence = cls._extract_emotion(content_lower, content)
        tags.extend(emotion_tags)
        
        # 3. ğŸ†• ç´§æ€¥åº¦ï¼ˆç‹¬ç«‹ç»´åº¦ï¼‰
        urgency = cls._extract_urgency(content_lower)
        
        # 4. ä»»åŠ¡ç±»å‹
        tags.extend(cls._extract_task_type(content_lower, metadata))
        
        # 5. è¯­è¨€æ£€æµ‹
        if cls._is_chinese_dominant(content):
            tags.append(ContextTags.LANG_ZH)
        else:
            tags.append(ContextTags.LANG_EN)
        
        return list(set(tags)), max(-1.0, min(1.0, valence)), urgency
    
    @classmethod
    def _extract_emotion(cls, content_lower: str, content_orig: str) -> Tuple[List[str], float]:
        """æå–æƒ…ç»ªæ ‡ç­¾å’Œæ•ˆä»·ï¼ˆå«å¦å®šè¯†åˆ«ï¼‰"""
        tags = []
        valence = 0.0
        
        # ğŸ†• æ£€æµ‹å¦å®šä¸Šä¸‹æ–‡
        has_negation = any(neg in content_lower for neg in cls.NEGATION_WORDS)
        
        positive_count = sum(1 for kw in cls.EMOTIONAL_KEYWORDS["positive"] if kw in content_lower)
        negative_count = sum(1 for kw in cls.EMOTIONAL_KEYWORDS["negative"] if kw in content_lower)
        
        # ğŸ†• å¦å®šç¿»è½¬é€»è¾‘
        if has_negation:
            # "æ²¡è§£å†³" = negativeï¼Œ"æ²¡é—®é¢˜" = positive
            positive_count, negative_count = negative_count, positive_count
        
        if positive_count > negative_count:
            valence = min(0.3 + 0.1 * positive_count, 1.0)
            tags.append(ContextTags.EMOTION_POSITIVE)
        elif negative_count > positive_count:
            valence = max(-0.3 - 0.1 * negative_count, -1.0)
            tags.append(ContextTags.EMOTION_NEGATIVE)
        
        return tags, valence
    
    @classmethod
    def _extract_urgency(cls, content_lower: str) -> int:
        """ğŸ†• æå–ç´§æ€¥åº¦ï¼ˆ0-3ï¼‰"""
        for level, keywords in cls.URGENCY_KEYWORDS.items():
            if any(kw.lower() in content_lower for kw in keywords):
                return level
        return 0
    
    @classmethod
    def _extract_time_context(cls, now: dt.datetime) -> List[str]:
        """æå–æ—¶é—´ä¸Šä¸‹æ–‡æ ‡ç­¾"""
        tags = []
        hour = now.hour
        
        if cls.WORK_HOURS[0] <= hour < cls.WORK_HOURS[1]:
            tags.append(ContextTags.TIME_WORK_HOURS)
        elif 22 <= hour or hour < 6:
            tags.append(ContextTags.TIME_LATE_NIGHT)
        elif 6 <= hour < 9:
            tags.append(ContextTags.TIME_EARLY_MORNING)
        else:
            tags.append(ContextTags.TIME_EVENING)
        
        if now.weekday() >= 5:
            tags.append(ContextTags.TIME_WEEKEND)
        
        return tags
    
    @classmethod
    def _extract_task_type(cls, content_lower: str, metadata: Dict) -> List[str]:
        """æå–ä»»åŠ¡ç±»å‹æ ‡ç­¾"""
        tags = []
        
        for tag, keywords in cls.TASK_KEYWORDS.items():
            if any(kw in content_lower for kw in keywords):
                tags.append(tag)
        
        return tags
    
    @staticmethod
    def _is_chinese_dominant(text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä»¥ä¸­æ–‡ä¸ºä¸»"""
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        total_chars = len(re.findall(r'\S', text))
        return chinese_chars > total_chars * 0.3 if total_chars > 0 else False
```

---

## ğŸ”— å…³è”è®°å¿†ï¼ˆPhase 2.2b - å¼‚æ­¥ï¼‰

### è®¾è®¡å˜æ›´

| åŸæ–¹æ¡ˆ | ä¿®è®¢æ–¹æ¡ˆ |
|--------|----------|
| `put()` ä¸­åŒæ­¥å»ºç«‹å…³è” | å¼‚æ­¥é˜Ÿåˆ—ï¼Œåå°ä»»åŠ¡ |
| `related_ids_json` = `["id1", "id2"]` | `related_json` = `[{"id": "id1", "score": 0.65}]` |
| ç¡¬ç¼–ç é˜ˆå€¼ 0.4/0.8 | åŸºäºåˆ†å¸ƒæ ¡å‡† |

### æ–‡ä»¶ï¼š`memtool/context/linker.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""è®°å¿†å…³è”å™¨ v2ï¼šå¼‚æ­¥å»ºç«‹ï¼Œå¸¦æƒé‡"""

from __future__ import annotations
import json
import logging
import threading
from typing import Dict, List, Optional, TYPE_CHECKING
from queue import Queue

if TYPE_CHECKING:
    from memtool_core import MemoryStore

LOG = logging.getLogger("memtool.linker")


class MemoryLinker:
    """å¼‚æ­¥è®°å¿†å…³è”å™¨"""
    
    # ğŸ†• åŠ¨æ€é˜ˆå€¼ï¼ˆå¯é€šè¿‡æ ¡å‡†è„šæœ¬æ›´æ–°ï¼‰
    LINK_THRESHOLD = 0.4       # ç›¸ä¼¼åº¦ä¸‹é™
    DUPLICATE_THRESHOLD = 0.8  # é‡å¤ä¸Šé™
    MAX_LINKS = 5
    
    def __init__(self, store: "MemoryStore"):
        self.store = store
        self._queue: Queue = Queue()
        self._worker: Optional[threading.Thread] = None
        self._running = False
    
    def start_worker(self):
        """å¯åŠ¨åå°å…³è”çº¿ç¨‹"""
        if self._worker and self._worker.is_alive():
            return
        self._running = True
        self._worker = threading.Thread(target=self._process_queue, daemon=True)
        self._worker.start()
        LOG.info("Linker worker started")
    
    def stop_worker(self):
        """åœæ­¢åå°çº¿ç¨‹"""
        self._running = False
        self._queue.put(None)  # å”¤é†’çº¿ç¨‹é€€å‡º
        if self._worker:
            self._worker.join(timeout=5)
    
    def enqueue(self, item_id: str, content: str, mem_type: str):
        """ğŸ†• åŠ å…¥å…³è”é˜Ÿåˆ—ï¼ˆéé˜»å¡ï¼‰"""
        self._queue.put({
            "id": item_id,
            "content": content,
            "type": mem_type
        })
    
    def _process_queue(self):
        """åå°å¤„ç†å…³è”ä»»åŠ¡"""
        while self._running:
            try:
                task = self._queue.get(timeout=1)
                if task is None:
                    break
                self._build_links(task["id"], task["content"], task["type"])
            except Exception as e:
                LOG.warning(f"Linker error: {e}")
    
    def _build_links(self, item_id: str, content: str, mem_type: str):
        """å»ºç«‹å…³è”ï¼ˆåå°æ‰§è¡Œï¼‰"""
        try:
            # æœç´¢ç›¸å…³è®°å¿†
            results = self.store.hybrid_search(
                query=content[:500],
                limit=self.MAX_LINKS * 2
            )
        except Exception:
            results = self.store.search(query=content[:200], limit=self.MAX_LINKS * 2)
        
        related = []
        for item in results.get("items", []):
            other_id = item.get("id")
            score = item.get("similarity", item.get("score", 0.5))
            
            if other_id == item_id:
                continue
            if score > self.DUPLICATE_THRESHOLD:
                continue
            if score >= self.LINK_THRESHOLD:
                related.append({"id": other_id, "score": round(score, 3)})
            
            if len(related) >= self.MAX_LINKS:
                break
        
        if related:
            # æ›´æ–°å½“å‰è®°å¿†çš„å…³è”
            self.store._update_related(item_id, related)
            # åŒå‘æ›´æ–°
            self._update_reverse_links(item_id, related)
    
    def _update_reverse_links(self, from_id: str, related: List[Dict]):
        """æ›´æ–°åå‘é“¾æ¥"""
        for rel in related:
            try:
                target = self.store.get(item_id=rel["id"])
                if not target or not target.get("ok"):
                    continue
                
                existing = target.get("related", [])
                # é¿å…é‡å¤
                if not any(r["id"] == from_id for r in existing):
                    existing.append({"id": from_id, "score": rel["score"]})
                    existing = existing[-self.MAX_LINKS:]  # ä¿ç•™æœ€æ–°
                    self.store._update_related(rel["id"], existing)
            except Exception as e:
                LOG.debug(f"Reverse link failed: {e}")


# ğŸ†• é˜ˆå€¼æ ¡å‡†å·¥å…·
def calibrate_thresholds(store: "MemoryStore", sample_size: int = 100) -> Dict:
    """
    åŸºäºç°æœ‰æ•°æ®æ ¡å‡†é˜ˆå€¼
    
    Returns:
        {"link_threshold": 0.xx, "duplicate_threshold": 0.xx}
    """
    from random import sample
    
    # è·å–æ ·æœ¬
    all_items = store.list(limit=sample_size * 2)
    items = all_items.get("items", [])[:sample_size]
    
    if len(items) < 10:
        return {"error": "Not enough data for calibration"}
    
    # è®¡ç®—ç›¸ä¼¼åº¦åˆ†å¸ƒ
    scores = []
    for i, item in enumerate(items[:20]):  # é™åˆ¶è®¡ç®—é‡
        try:
            results = store.hybrid_search(query=item["content"][:200], limit=10)
            for r in results.get("items", []):
                if r["id"] != item["id"]:
                    scores.append(r.get("similarity", r.get("score", 0.5)))
        except Exception:
            continue
    
    if not scores:
        return {"error": "No similarity scores collected"}
    
    scores.sort()
    
    # åŸºäºåˆ†ä½æ•°è®¾ç½®é˜ˆå€¼
    p10 = scores[int(len(scores) * 0.1)]
    p90 = scores[int(len(scores) * 0.9)]
    
    return {
        "link_threshold": round(p10, 2),       # top 10% ä½œä¸ºç›¸å…³
        "duplicate_threshold": round(p90, 2),  # top 10% ä½œä¸ºé‡å¤
        "sample_count": len(scores),
        "score_range": [round(min(scores), 2), round(max(scores), 2)]
    }
```

---

## ğŸ“Š è§‚æµ‹æŒ‡æ ‡ï¼ˆPhase 2.2bï¼‰

### æ–°å¢ç»Ÿè®¡é¡¹

```python
# åœ¨ memtool/observability.py ä¸­æ·»åŠ 

def compute_context_stats(db_path: str) -> Dict:
    """æƒ…å¢ƒè®°å¿†ç»Ÿè®¡"""
    conn = sqlite3.connect(db_path)
    
    stats = {}
    
    # æ ‡ç­¾åˆ†å¸ƒ
    rows = conn.execute("""
        SELECT context_tags_json FROM memory_items 
        WHERE context_tags_json != '[]'
    """).fetchall()
    
    tag_counts = {}
    for row in rows:
        tags = json.loads(row[0])
        for tag in tags:
            tag_counts[tag] = tag_counts.get(tag, 0) + 1
    
    stats["tag_distribution"] = tag_counts
    
    # æƒ…ç»ªåˆ†å¸ƒ
    stats["valence_distribution"] = {
        "positive": conn.execute("SELECT COUNT(*) FROM memory_items WHERE emotional_valence > 0").fetchone()[0],
        "neutral": conn.execute("SELECT COUNT(*) FROM memory_items WHERE emotional_valence = 0").fetchone()[0],
        "negative": conn.execute("SELECT COUNT(*) FROM memory_items WHERE emotional_valence < 0").fetchone()[0],
    }
    
    # å…³è”ç»Ÿè®¡
    stats["linking"] = {
        "with_links": conn.execute("SELECT COUNT(*) FROM memory_items WHERE related_json != '[]'").fetchone()[0],
        "avg_links": conn.execute("SELECT AVG(json_array_length(related_json)) FROM memory_items WHERE related_json != '[]'").fetchone()[0] or 0,
    }
    
    conn.close()
    return stats
```

---

## âœ… å®æ–½æ¸…å•ï¼ˆä¿®è®¢ç‰ˆï¼‰

### Phase 2.2aï¼ˆåŒæ­¥ï¼Œä¼˜å…ˆï¼‰

| # | ä»»åŠ¡ | é¢„ä¼° | ä¼˜å…ˆçº§ |
|---|------|------|--------|
| 1 | æ•°æ®åº“è¿ç§»ï¼ˆ5 ä¸ªå­—æ®µ + 3 ä¸ªç´¢å¼•ï¼‰ | 15min | P0 |
| 2 | ContextTags å¸¸é‡å®šä¹‰ | 10min | P0 |
| 3 | ContextExtractor v2ï¼ˆå«å¦å®šè¯†åˆ«ï¼‰ | 45min | P0 |
| 4 | ä¿®æ”¹ `put()` æå–æƒ…å¢ƒï¼ˆä¸å«å…³è”ï¼‰ | 20min | P0 |
| 5 | `memory_contextual_search` MCP Tool | 30min | P0 |
| 6 | `memory_parse_context` MCP Tool | 20min | P1 |
| 7 | æµ‹è¯• `test_phase2_2a.py` | 30min | P0 |

**å°è®¡ï¼š~2.5 å°æ—¶**

### Phase 2.2bï¼ˆå¼‚æ­¥ï¼Œåç»­ï¼‰

| # | ä»»åŠ¡ | é¢„ä¼° | ä¼˜å…ˆçº§ |
|---|------|------|--------|
| 1 | MemoryLinker v2ï¼ˆå¼‚æ­¥é˜Ÿåˆ—ï¼‰ | 45min | P0 |
| 2 | é˜ˆå€¼æ ¡å‡†è„šæœ¬ | 30min | P1 |
| 3 | è§‚æµ‹æŒ‡æ ‡ `compute_context_stats` | 20min | P1 |
| 4 | é›†æˆåˆ° `put()` çš„ enqueue è°ƒç”¨ | 15min | P0 |
| 5 | æµ‹è¯• `test_phase2_2b.py` | 30min | P0 |

**å°è®¡ï¼š~2.5 å°æ—¶**

---

## ğŸ¯ éªŒæ”¶æ ‡å‡†ï¼ˆä¿®è®¢ç‰ˆï¼‰

### Phase 2.2a

1. âœ… æ–°è®°å¿†è‡ªåŠ¨å¸¦ä¸Š `time:xxx`/`task:xxx`/`emotion:xxx` æ ‡ç­¾
2. âœ… "æ²¡è§£å†³" æ­£ç¡®è¯†åˆ«ä¸º negativeï¼ˆå¦å®šè¯†åˆ«ï¼‰
3. âœ… `memory_contextual_search` èƒ½æŒ‰æ ‡ç­¾å’Œæƒ…ç»ªè¿‡æ»¤
4. âœ… "æ˜¨æ™šé‚£ä¸ª OOM" èƒ½è§£æå¹¶æ£€ç´¢
5. âœ… `put()` å†™å…¥å»¶è¿Ÿå¢åŠ  < 5ms
6. âœ… æ‰€æœ‰ç°æœ‰æµ‹è¯•é€šè¿‡

### Phase 2.2b

1. âœ… å…³è”åœ¨åå°å¼‚æ­¥å»ºç«‹
2. âœ… `related_json` å¸¦æƒé‡ `[{id, score}]`
3. âœ… é˜ˆå€¼å¯é€šè¿‡ `calibrate_thresholds()` æ ¡å‡†
4. âœ… è§‚æµ‹æŒ‡æ ‡å¯ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒã€æƒ…ç»ªåˆ†å¸ƒã€å…³è”ç‡

---

## ğŸ”„ ä¸åŸæ–¹æ¡ˆå¯¹æ¯”

| ç»´åº¦ | åŸæ–¹æ¡ˆ | ä¿®è®¢æ–¹æ¡ˆ |
|------|--------|----------|
| å®æ–½æ–¹å¼ | ä¸€æ¬¡æ€§ | åˆ† 2.2a + 2.2b |
| å…³è”å»ºç«‹ | åŒæ­¥ `put()` | å¼‚æ­¥é˜Ÿåˆ— |
| ç´§æ€¥åº¦ | æ··å…¥ valence | ç‹¬ç«‹ç»´åº¦ |
| å…³è”æ ¼å¼ | `["id"]` | `[{id, score}]` |
| æ ‡ç­¾å‘½å | ä¸ç»Ÿä¸€ | `prefix:name` ç»Ÿä¸€ |
| å¦å®šè¯†åˆ« | æ—  | æœ‰ |
| é˜ˆå€¼ | ç¡¬ç¼–ç  | å¯æ ¡å‡† |
| æ—¶é—´ä¼°ç®— | 3.5hï¼ˆä¹è§‚ï¼‰ | 5hï¼ˆç°å®ï¼‰ |

---

_ä¿®è®¢è€…ï¼šOpusCoder + Codex_
_ä¿®è®¢æ—¶é—´ï¼š2026-02-03 16:30 GMT+8_
