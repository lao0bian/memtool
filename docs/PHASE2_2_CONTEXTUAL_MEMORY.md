# Phase 2-2: æƒ…å¢ƒè®°å¿†æŠ€æœ¯æ–¹æ¡ˆ

## ğŸ“‹ è®¾è®¡ç›®æ ‡

è®©è®°å¿†ç³»ç»Ÿä»"æˆ‘è®°ä½äº†ä»€ä¹ˆ"å‡çº§åˆ°"æˆ‘åœ¨ä»€ä¹ˆæƒ…å¢ƒä¸‹è®°ä½äº†ä»€ä¹ˆ"ã€‚

| èƒ½åŠ› | å½“å‰ | ç›®æ ‡ |
|------|------|------|
| æ—¶é—´æ„ŸçŸ¥ | âŒ åªæœ‰ `created_at` | âœ… å·¥ä½œæ—¶é—´/æ·±å¤œ/å‘¨æœ«æ ‡ç­¾ |
| æƒ…ç»ªæ ‡è®° | âŒ æ—  | âœ… æ­£å‘ç»éªŒ/è´Ÿå‘æ•™è®­/ç´§æ€¥ |
| å…³è”è®°å¿† | âŒ æ—  | âœ… è‡ªåŠ¨å»ºç«‹ç›¸å…³è®°å¿†é“¾æ¥ |
| æƒ…å¢ƒæ£€ç´¢ | âŒ æ—  | âœ… "æ˜¨æ™šé‚£ä¸ª bug" ç›´æ¥å‘½ä¸­ |

---

## ğŸ—„ï¸ æ•°æ®åº“æ‰©å±•

### æ–°å¢å­—æ®µ

```sql
-- Phase 2-2: æƒ…å¢ƒè®°å¿†å­—æ®µ
ALTER TABLE memory_items ADD COLUMN context_tags_json TEXT NOT NULL DEFAULT '[]';
ALTER TABLE memory_items ADD COLUMN emotional_valence REAL NOT NULL DEFAULT 0.0;
ALTER TABLE memory_items ADD COLUMN related_ids_json TEXT NOT NULL DEFAULT '[]';
ALTER TABLE memory_items ADD COLUMN session_id TEXT;

-- ç´¢å¼•
CREATE INDEX IF NOT EXISTS idx_memory_emotional ON memory_items(emotional_valence);
CREATE INDEX IF NOT EXISTS idx_memory_session ON memory_items(session_id);
```

### å­—æ®µè¯´æ˜

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `context_tags_json` | TEXT | JSON æ•°ç»„ï¼Œå¦‚ `["work_hours", "debugging", "urgent"]` |
| `emotional_valence` | REAL | æƒ…æ„Ÿæ•ˆä»· -1.0 ~ +1.0ï¼ˆè´Ÿ=æ•™è®­ï¼Œæ­£=æˆåŠŸç»éªŒï¼‰ |
| `related_ids_json` | TEXT | JSON æ•°ç»„ï¼Œç›¸å…³è®°å¿† ID åˆ—è¡¨ |
| `session_id` | TEXT | æ¥æºä¼šè¯æ ‡è¯†ï¼ˆå¯è¿½æº¯å¯¹è¯ä¸Šä¸‹æ–‡ï¼‰ |

---

## ğŸ§  ä¸Šä¸‹æ–‡æå–å™¨

### æ–‡ä»¶ï¼š`memtool/context/extractor.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""æƒ…å¢ƒæå–å™¨ï¼šè‡ªåŠ¨ä»è®°å¿†å†…å®¹ä¸­æå–ä¸Šä¸‹æ–‡æ ‡ç­¾å’Œæƒ…æ„Ÿæ•ˆä»·"""

from __future__ import annotations
import datetime as dt
import re
from typing import Dict, List, Tuple, Optional

class ContextExtractor:
    """è‡ªåŠ¨æå–è®°å¿†çš„ä¸Šä¸‹æ–‡æ ‡ç­¾å’Œæƒ…æ„Ÿæ•ˆä»·"""
    
    # æƒ…ç»ªå…³é”®è¯åº“ï¼ˆä¸­è‹±åŒè¯­ï¼‰
    EMOTIONAL_KEYWORDS = {
        "positive": [
            "success", "solved", "fixed", "completed", "optimized", "improved",
            "æˆåŠŸ", "è§£å†³", "ä¿®å¤", "å®Œæˆ", "ä¼˜åŒ–", "æ”¹è¿›", "æå®š", "é€šè¿‡"
        ],
        "negative": [
            "error", "failed", "bug", "issue", "timeout", "crash", "exception",
            "é”™è¯¯", "å¤±è´¥", "é—®é¢˜", "è¶…æ—¶", "å´©æºƒ", "å¼‚å¸¸", "æŠ¥é”™", "å¡ä½"
        ],
        "urgent": [
            "urgent", "critical", "blocking", "asap", "immediately",
            "ç´§æ€¥", "å…³é”®", "é˜»å¡", "ç«‹å³", "é©¬ä¸Š", "P0"
        ],
    }
    
    # ä»»åŠ¡ç±»å‹å…³é”®è¯
    TASK_KEYWORDS = {
        "debugging": ["debug", "trace", "stack", "è°ƒè¯•", "æ’æŸ¥", "å®šä½"],
        "api_design": ["api", "endpoint", "rest", "graphql", "æ¥å£"],
        "data_model": ["schema", "database", "table", "migration", "æ•°æ®åº“", "è¡¨ç»“æ„"],
        "refactor": ["refactor", "cleanup", "é‡æ„", "æ•´ç†", "ä¼˜åŒ–ç»“æ„"],
        "testing": ["test", "unittest", "pytest", "æµ‹è¯•", "ç”¨ä¾‹"],
        "deployment": ["deploy", "release", "docker", "k8s", "éƒ¨ç½²", "å‘å¸ƒ"],
    }
    
    WORK_HOURS = (9, 18)  # å·¥ä½œæ—¶é—´ 9:00-18:00
    
    @classmethod
    def extract(
        cls,
        content: str,
        metadata: Optional[Dict] = None,
        timestamp: Optional[dt.datetime] = None
    ) -> Tuple[List[str], float]:
        """
        æå–ä¸Šä¸‹æ–‡æ ‡ç­¾å’Œæƒ…æ„Ÿæ•ˆä»·
        
        Args:
            content: è®°å¿†å†…å®¹
            metadata: å…ƒæ•°æ®ï¼ˆtype, task_id ç­‰ï¼‰
            timestamp: æ—¶é—´æˆ³ï¼Œé»˜è®¤å½“å‰æ—¶é—´
        
        Returns:
            (context_tags, emotional_valence)
        """
        metadata = metadata or {}
        now = timestamp or dt.datetime.now()
        
        tags = []
        valence = 0.0
        
        content_lower = content.lower()
        
        # 1. æ—¶é—´ä¸Šä¸‹æ–‡
        tags.extend(cls._extract_time_context(now))
        
        # 2. æƒ…ç»ªæ£€æµ‹
        emotion_tags, valence = cls._extract_emotion(content_lower)
        tags.extend(emotion_tags)
        
        # 3. ä»»åŠ¡ç±»å‹æ¨æ–­
        tags.extend(cls._extract_task_type(content_lower, metadata))
        
        # 4. è¯­è¨€æ£€æµ‹
        if cls._is_chinese_dominant(content):
            tags.append("lang:zh")
        else:
            tags.append("lang:en")
        
        # å»é‡å¹¶è¿”å›
        return list(set(tags)), max(-1.0, min(1.0, valence))
    
    @classmethod
    def _extract_time_context(cls, now: dt.datetime) -> List[str]:
        """æå–æ—¶é—´ä¸Šä¸‹æ–‡æ ‡ç­¾"""
        tags = []
        hour = now.hour
        
        if cls.WORK_HOURS[0] <= hour < cls.WORK_HOURS[1]:
            tags.append("time:work_hours")
        elif 22 <= hour or hour < 6:
            tags.append("time:late_night")
        elif 6 <= hour < 9:
            tags.append("time:early_morning")
        else:
            tags.append("time:evening")
        
        if now.weekday() >= 5:
            tags.append("time:weekend")
        
        return tags
    
    @classmethod
    def _extract_emotion(cls, content_lower: str) -> Tuple[List[str], float]:
        """æå–æƒ…ç»ªæ ‡ç­¾å’Œæ•ˆä»·"""
        tags = []
        valence = 0.0
        
        positive_count = sum(1 for kw in cls.EMOTIONAL_KEYWORDS["positive"] if kw in content_lower)
        negative_count = sum(1 for kw in cls.EMOTIONAL_KEYWORDS["negative"] if kw in content_lower)
        urgent_count = sum(1 for kw in cls.EMOTIONAL_KEYWORDS["urgent"] if kw in content_lower)
        
        if positive_count > negative_count:
            valence = min(0.3 + 0.1 * positive_count, 1.0)
            tags.append("emotion:positive")
        elif negative_count > positive_count:
            valence = max(-0.3 - 0.1 * negative_count, -1.0)
            tags.append("emotion:negative")
        
        if urgent_count > 0:
            valence += 0.2  # ç´§æ€¥äº‹é¡¹æ›´é‡è¦
            tags.append("emotion:urgent")
        
        return tags, valence
    
    @classmethod
    def _extract_task_type(cls, content_lower: str, metadata: Dict) -> List[str]:
        """æå–ä»»åŠ¡ç±»å‹æ ‡ç­¾"""
        tags = []
        
        for task_type, keywords in cls.TASK_KEYWORDS.items():
            if any(kw in content_lower for kw in keywords):
                tags.append(f"task:{task_type}")
        
        # æ ¹æ® metadata.type è¡¥å……
        mem_type = metadata.get("type")
        if mem_type == "run":
            tags.append("scope:execution")
        elif mem_type == "feature":
            tags.append("scope:development")
        elif mem_type == "project":
            tags.append("scope:project")
        
        return tags
    
    @staticmethod
    def _is_chinese_dominant(text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä»¥ä¸­æ–‡ä¸ºä¸»"""
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        total_chars = len(re.findall(r'\S', text))
        return chinese_chars > total_chars * 0.3 if total_chars > 0 else False
```

---

## ğŸ”— å…³è”è®°å¿†å»ºç«‹

### æ–‡ä»¶ï¼š`memtool/context/linker.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""è®°å¿†å…³è”å™¨ï¼šè‡ªåŠ¨å»ºç«‹ç›¸å…³è®°å¿†ä¹‹é—´çš„é“¾æ¥"""

from __future__ import annotations
from typing import Dict, List, Optional, TYPE_CHECKING

if TYPE_CHECKING:
    from memtool_core import MemoryStore


class MemoryLinker:
    """è‡ªåŠ¨å»ºç«‹è®°å¿†å…³è”"""
    
    # ç›¸ä¼¼åº¦é˜ˆå€¼
    LINK_THRESHOLD = 0.4      # ä½äºæ­¤å€¼æ‰å»ºç«‹é“¾æ¥ï¼ˆå¤ªç›¸ä¼¼=é‡å¤ï¼‰
    DUPLICATE_THRESHOLD = 0.8 # é«˜äºæ­¤å€¼=é‡å¤ï¼Œä¸é“¾æ¥
    MAX_LINKS = 5             # æ¯æ¡è®°å¿†æœ€å¤šå…³è”æ•°
    
    def __init__(self, store: "MemoryStore"):
        self.store = store
    
    def find_related(
        self,
        content: str,
        mem_type: str,
        exclude_id: Optional[str] = None,
    ) -> List[str]:
        """
        æŸ¥æ‰¾ä¸ç»™å®šå†…å®¹ç›¸å…³çš„è®°å¿† ID
        
        Returns:
            ç›¸å…³è®°å¿† ID åˆ—è¡¨ï¼ˆä¸åŒ…å«é‡å¤é¡¹ï¼‰
        """
        try:
            # ä¼˜å…ˆä½¿ç”¨å‘é‡æœç´¢
            results = self.store.hybrid_search(
                query=content[:500],  # æˆªæ–­é¿å…è¿‡é•¿
                limit=self.MAX_LINKS * 2
            )
        except Exception:
            # é™çº§åˆ°æ™®é€šæœç´¢
            results = self.store.search(
                query=content[:200],
                limit=self.MAX_LINKS * 2
            )
        
        related_ids = []
        for item in results.get("items", []):
            item_id = item.get("id")
            similarity = item.get("similarity", item.get("score", 0.5))
            
            # æ’é™¤è‡ªå·±
            if item_id == exclude_id:
                continue
            
            # å¤ªç›¸ä¼¼=é‡å¤ï¼Œè·³è¿‡
            if similarity > self.DUPLICATE_THRESHOLD:
                continue
            
            # ç›¸ä¼¼åº¦é€‚ä¸­=ç›¸å…³
            if similarity >= self.LINK_THRESHOLD:
                related_ids.append(item_id)
            
            if len(related_ids) >= self.MAX_LINKS:
                break
        
        return related_ids
    
    def update_bidirectional_links(
        self,
        from_id: str,
        to_ids: List[str]
    ) -> int:
        """
        å»ºç«‹åŒå‘é“¾æ¥ï¼ˆAâ†’B æ—¶ä¹Ÿæ›´æ–° Bâ†’Aï¼‰
        
        Returns:
            æ›´æ–°çš„é“¾æ¥æ•°
        """
        updated = 0
        
        for to_id in to_ids:
            try:
                # è·å–ç›®æ ‡è®°å¿†çš„ç°æœ‰é“¾æ¥
                target = self.store.get(item_id=to_id)
                if not target or not target.get("ok"):
                    continue
                
                existing_links = target.get("related_ids", [])
                
                # å¦‚æœè¿˜æ²¡æœ‰åå‘é“¾æ¥ï¼Œæ·»åŠ å®ƒ
                if from_id not in existing_links:
                    existing_links.append(from_id)
                    # é™åˆ¶æœ€å¤§é“¾æ¥æ•°
                    existing_links = existing_links[-self.MAX_LINKS:]
                    
                    self.store._update_related_ids(to_id, existing_links)
                    updated += 1
            except Exception:
                continue
        
        return updated
```

---

## ğŸ” æƒ…å¢ƒæ£€ç´¢

### æ–°å¢ MCP Toolï¼š`memory_contextual_search`

```python
@mcp.tool()
def memory_contextual_search(
    query: str,
    context_tags: Optional[List[str]] = None,
    emotional_filter: Optional[str] = None,
    time_filter: Optional[str] = None,
    limit: int = 10,
    db_path: Optional[str] = None,
) -> Dict[str, Any]:
    """æƒ…å¢ƒæ£€ç´¢ï¼šåŸºäºä¸Šä¸‹æ–‡å’Œæƒ…ç»ªè¿‡æ»¤
    
    Args:
        query: æœç´¢å…³é”®è¯
        context_tags: ä¸Šä¸‹æ–‡æ ‡ç­¾è¿‡æ»¤ï¼Œå¦‚ ["debugging", "late_night"]
        emotional_filter: æƒ…ç»ªè¿‡æ»¤ (positive/negative/urgent)
        time_filter: æ—¶é—´è¿‡æ»¤ (work_hours/late_night/weekend)
        limit: è¿”å›æ•°é‡é™åˆ¶
        db_path: æ•°æ®åº“è·¯å¾„
    
    Examples:
        - "æ˜¨æ™šè°ƒè¯•çš„é‚£ä¸ªé—®é¢˜"
          â†’ context_tags=["time:late_night", "task:debugging"]
        - "ä¸Šæ¬¡æˆåŠŸè§£å†³çš„ç±»ä¼¼é—®é¢˜"
          â†’ emotional_filter="positive"
    """
    store = _store_for(db_path)
    
    # 1. åŸºç¡€æ£€ç´¢ï¼ˆå¤šå–ä¸€äº›ç”¨äºåç»­è¿‡æ»¤ï¼‰
    try:
        base_results = store.hybrid_search(query=query, limit=limit * 3)
    except Exception:
        base_results = store.search(query=query, limit=limit * 3)
    
    items = base_results.get("items", [])
    
    # 2. ä¸Šä¸‹æ–‡è¿‡æ»¤
    filtered = []
    for item in items:
        item_tags = item.get("context_tags", [])
        item_valence = item.get("emotional_valence", 0.0)
        
        # æ ‡ç­¾åŒ¹é…
        if context_tags:
            # è®¡ç®—æ ‡ç­¾é‡å åº¦
            overlap = len(set(context_tags) & set(item_tags))
            if overlap == 0:
                continue
            item["context_match_score"] = overlap / len(context_tags)
        
        # æ—¶é—´è¿‡æ»¤
        if time_filter:
            time_tag = f"time:{time_filter}"
            if time_tag not in item_tags:
                continue
        
        # æƒ…ç»ªè¿‡æ»¤
        if emotional_filter:
            if emotional_filter == "positive" and item_valence <= 0:
                continue
            if emotional_filter == "negative" and item_valence >= 0:
                continue
            if emotional_filter == "urgent" and "emotion:urgent" not in item_tags:
                continue
        
        filtered.append(item)
    
    # 3. æŒ‰ä¸Šä¸‹æ–‡åŒ¹é…åº¦é‡æ’åº
    if context_tags:
        filtered.sort(
            key=lambda x: x.get("context_match_score", 0),
            reverse=True
        )
    
    return {
        "ok": True,
        "items": filtered[:limit],
        "total_found": len(filtered),
        "filters_applied": {
            "context_tags": context_tags,
            "emotional_filter": emotional_filter,
            "time_filter": time_filter
        }
    }
```

---

## ğŸ“Š è‡ªç„¶è¯­è¨€æƒ…å¢ƒè§£æ

### æ–°å¢ MCP Toolï¼š`memory_parse_context`

```python
@mcp.tool()
def memory_parse_context(
    natural_query: str,
    db_path: Optional[str] = None,
) -> Dict[str, Any]:
    """è§£æè‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œæå–æƒ…å¢ƒæ¡ä»¶
    
    å°† "æ˜¨æ™šè°ƒçš„é‚£ä¸ª OOM" è½¬æ¢ä¸ºç»“æ„åŒ–æŸ¥è¯¢æ¡ä»¶
    
    Returns:
        {
            "query": "OOM",
            "context_tags": ["time:late_night"],
            "emotional_filter": null,
            "suggested_search": {...}
        }
    """
    import re
    
    query_text = natural_query
    context_tags = []
    emotional_filter = None
    
    # æ—¶é—´è¯å…¸
    TIME_PATTERNS = {
        r"æ˜¨æ™š|æ˜¨å¤©æ™šä¸Š|last\s+night": "time:late_night",
        r"ä»Šæ—©|ä»Šå¤©æ—©ä¸Š|this\s+morning": "time:early_morning",
        r"å‘¨æœ«|weekend": "time:weekend",
        r"ä¸Šç­æ—¶é—´|å·¥ä½œæ—¶é—´|work\s+hours?": "time:work_hours",
    }
    
    # æƒ…ç»ªè¯å…¸
    EMOTION_PATTERNS = {
        r"æˆåŠŸ|è§£å†³äº†|æå®š|succeeded?|fixed": "positive",
        r"å¤±è´¥|æ²¡æå®š|é—®é¢˜|failed|broken": "negative",
        r"ç´§æ€¥|é©¬ä¸Š|urgent|asap": "urgent",
    }
    
    # ä»»åŠ¡è¯å…¸
    TASK_PATTERNS = {
        r"è°ƒè¯•|debug": "task:debugging",
        r"æµ‹è¯•|test": "task:testing",
        r"éƒ¨ç½²|deploy": "task:deployment",
        r"é‡æ„|refactor": "task:refactor",
    }
    
    # æå–æ—¶é—´ä¸Šä¸‹æ–‡
    for pattern, tag in TIME_PATTERNS.items():
        if re.search(pattern, natural_query, re.IGNORECASE):
            context_tags.append(tag)
            query_text = re.sub(pattern, "", query_text, flags=re.IGNORECASE)
    
    # æå–æƒ…ç»ªè¿‡æ»¤
    for pattern, emotion in EMOTION_PATTERNS.items():
        if re.search(pattern, natural_query, re.IGNORECASE):
            emotional_filter = emotion
            query_text = re.sub(pattern, "", query_text, flags=re.IGNORECASE)
            break
    
    # æå–ä»»åŠ¡ç±»å‹
    for pattern, tag in TASK_PATTERNS.items():
        if re.search(pattern, natural_query, re.IGNORECASE):
            context_tags.append(tag)
    
    # æ¸…ç†æŸ¥è¯¢æ–‡æœ¬
    query_text = re.sub(r"[é‚£ä¸ª|çš„|è¿™ä¸ª|ä¸Šæ¬¡|ä¹‹å‰|that|the|this]", "", query_text)
    query_text = query_text.strip()
    
    return {
        "ok": True,
        "original_query": natural_query,
        "parsed": {
            "query": query_text or natural_query,
            "context_tags": context_tags,
            "emotional_filter": emotional_filter,
        },
        "suggested_call": {
            "tool": "memory_contextual_search",
            "args": {
                "query": query_text or natural_query,
                "context_tags": context_tags if context_tags else None,
                "emotional_filter": emotional_filter,
            }
        }
    }
```

---

## ğŸ”§ æ ¸å¿ƒæ¨¡å—ä¿®æ”¹

### `memtool_core.py` ä¿®æ”¹

```python
# åœ¨ put() æ–¹æ³•ä¸­é›†æˆæƒ…å¢ƒæå–

def put(
    self,
    type: str,
    key: str,
    content: str,
    ...,
    session_id: Optional[str] = None,  # æ–°å¢å‚æ•°
    auto_link: bool = True,            # æ–°å¢å‚æ•°
) -> Dict[str, Any]:
    """å†™å…¥è®°å¿†ï¼Œè‡ªåŠ¨æå–æƒ…å¢ƒå’Œå»ºç«‹å…³è”"""
    
    # ... åŸæœ‰é€»è¾‘ ...
    
    # Phase 2-2: æƒ…å¢ƒæå–
    from memtool.context.extractor import ContextExtractor
    context_tags, emotional_valence = ContextExtractor.extract(
        content=content,
        metadata={"type": type, "task_id": task_id},
    )
    
    # Phase 2-2: è‡ªåŠ¨å…³è”
    related_ids = []
    if auto_link:
        from memtool.context.linker import MemoryLinker
        linker = MemoryLinker(self)
        related_ids = linker.find_related(
            content=content,
            mem_type=type,
            exclude_id=final_id,
        )
    
    # æ›´æ–° SQL
    conn.execute("""
        UPDATE memory_items
        SET context_tags_json = ?,
            emotional_valence = ?,
            related_ids_json = ?,
            session_id = ?
        WHERE id = ?
    """, (
        json.dumps(context_tags, ensure_ascii=False),
        emotional_valence,
        json.dumps(related_ids, ensure_ascii=False),
        session_id,
        final_id
    ))
    
    # å»ºç«‹åŒå‘é“¾æ¥
    if auto_link and related_ids:
        linker.update_bidirectional_links(final_id, related_ids)
    
    # ... è¿”å›ç»“æœ ...
```

---

## ğŸ“ ç›®å½•ç»“æ„

```
memtool_mvp/
â”œâ”€â”€ memtool/
â”‚   â”œâ”€â”€ context/              # æ–°å¢ç›®å½•
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ extractor.py      # ä¸Šä¸‹æ–‡æå–å™¨
â”‚   â”‚   â””â”€â”€ linker.py         # è®°å¿†å…³è”å™¨
â”‚   â”œâ”€â”€ embedding/            # å·²æœ‰
â”‚   â””â”€â”€ ...
â”œâ”€â”€ mcp_server.py             # æ–°å¢ MCP tools
â”œâ”€â”€ memtool_core.py           # ä¿®æ”¹ put() æ–¹æ³•
â””â”€â”€ test_phase2_2.py          # æ–°å¢æµ‹è¯•
```

---

## âœ… å®æ–½æ¸…å•

| # | ä»»åŠ¡ | é¢„ä¼° | ä¼˜å…ˆçº§ |
|---|------|------|--------|
| 1 | æ•°æ®åº“è¿ç§»ï¼ˆæ–°å¢ 4 ä¸ªå­—æ®µ + 2 ä¸ªç´¢å¼•ï¼‰ | 15min | P0 |
| 2 | å®ç° `context/extractor.py` | 45min | P0 |
| 3 | å®ç° `context/linker.py` | 30min | P0 |
| 4 | ä¿®æ”¹ `memtool_core.py` çš„ `put()` | 30min | P0 |
| 5 | æ–°å¢ MCP Tool `memory_contextual_search` | 30min | P0 |
| 6 | æ–°å¢ MCP Tool `memory_parse_context` | 20min | P1 |
| 7 | ç¼–å†™ `test_phase2_2.py` æµ‹è¯• | 40min | P0 |
| 8 | æ›´æ–° README æ–‡æ¡£ | 15min | P1 |

**æ€»è®¡ï¼šçº¦ 3.5 å°æ—¶**

---

## ğŸ¯ éªŒæ”¶æ ‡å‡†

1. âœ… æ–°è®°å¿†è‡ªåŠ¨å¸¦ä¸Šæ—¶é—´/æƒ…ç»ª/ä»»åŠ¡ç±»å‹æ ‡ç­¾
2. âœ… ç›¸å…³è®°å¿†è‡ªåŠ¨å»ºç«‹åŒå‘é“¾æ¥
3. âœ… `memory_contextual_search` èƒ½æŒ‰æ ‡ç­¾å’Œæƒ…ç»ªè¿‡æ»¤
4. âœ… "æ˜¨æ™šé‚£ä¸ª OOM" èƒ½æ­£ç¡®è§£æå¹¶æ£€ç´¢
5. âœ… æ‰€æœ‰ç°æœ‰æµ‹è¯•ä»é€šè¿‡
6. âœ… æ€§èƒ½æ— æ˜æ˜¾é€€åŒ–ï¼ˆ< 20ms é¢å¤–å»¶è¿Ÿï¼‰

---

## ğŸš¨ é£é™©ä¸ç¼“è§£

| é£é™© | ç¼“è§£æªæ–½ |
|------|----------|
| æƒ…ç»ªæ£€æµ‹ä¸å‡†ç¡® | ä¿å®ˆç­–ç•¥ï¼ˆåªåœ¨æ˜ç¡®æ—¶æ ‡è®°ï¼‰ï¼Œæ”¯æŒæ‰‹åŠ¨è¦†ç›– |
| å…³è”å»ºç«‹è€—æ—¶ | å¼‚æ­¥å¤„ç†ï¼Œä½¿ç”¨å†…å®¹æˆªæ–­ï¼ˆ500å­—ç¬¦ï¼‰ |
| å‘åå…¼å®¹é—®é¢˜ | æ‰€æœ‰æ–°å­—æ®µæœ‰é»˜è®¤å€¼ï¼Œæ—§æ•°æ®ä¸å—å½±å“ |
| å‘é‡æœç´¢ä¸å¯ç”¨ | è‡ªåŠ¨é™çº§åˆ° FTS5 æœç´¢ |

---

## ğŸ“ˆ åç»­è¿­ä»£

### Phase 2-2.1: ä¸Šä¸‹æ–‡å¢å¼º
- æ”¯æŒè‡ªå®šä¹‰æ ‡ç­¾ï¼ˆç”¨æˆ·æ‰‹åŠ¨æ·»åŠ ï¼‰
- åŸºäº LLM çš„æ™ºèƒ½æƒ…ç»ªåˆ†æ
- æ—¶åŒºæ„ŸçŸ¥

### Phase 2-2.2: å…³è”å¢å¼º
- å›¾æ•°æ®åº“å¯è§†åŒ–
- å…³è”å¼ºåº¦æƒé‡
- è·¨é¡¹ç›®å…³è”

---

_è®¾è®¡è€…ï¼šOpusCoder_
_åˆ›å»ºæ—¶é—´ï¼š2026-02-03 14:05 GMT+8_
_å¾…è¯„å®¡ï¼šCodex_
